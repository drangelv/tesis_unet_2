{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5238431",
   "metadata": {},
   "source": [
    "# Pruebas del Modelo UNet3 para Predicción de Heatmaps\n",
    "\n",
    "Este notebook permite cargar un modelo entrenado y realizar pruebas adicionales, visualizando los resultados y calculando métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Agregar el directorio src al path para poder importar los módulos\n",
    "sys.path.append('..')\n",
    "from src.models.unet3 import UNet3\n",
    "from src.data.dataset import HeatmapDataset\n",
    "from src.metrics.metrics import calculate_metrics\n",
    "from config.config import MODEL_CONFIG, DATA_CONFIG, METRICS_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463a450",
   "metadata": {},
   "source": [
    "## 1. Cargar Modelo Entrenado\n",
    "\n",
    "Primero cargamos el modelo entrenado desde los checkpoints guardados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eed238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path):\n",
    "    \"\"\"Carga el modelo desde un checkpoint\"\"\"\n",
    "    model = UNet3(\n",
    "        n_channels=MODEL_CONFIG['input_frames'],\n",
    "        n_classes=MODEL_CONFIG['output_frames']\n",
    "    )\n",
    "    \n",
    "    # Determinar dispositivo\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    \n",
    "    # Cargar checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Modelo cargado desde: {checkpoint_path}\")\n",
    "    print(f\"Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Ruta al mejor modelo (ajusta según tu estructura)\n",
    "checkpoint_path = \"../saved_models/best_model.ckpt\"\n",
    "model = load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f7cc4",
   "metadata": {},
   "source": [
    "## 2. Cargar Datos de Test\n",
    "\n",
    "Cargamos el conjunto de datos de test para realizar las pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26711b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset completo\n",
    "test_dataset = HeatmapDataset(\n",
    "    file_path=DATA_CONFIG['data_path'],\n",
    "    input_frames=MODEL_CONFIG['input_frames'],\n",
    "    output_frames=MODEL_CONFIG['output_frames'],\n",
    "    subset='test'\n",
    ")\n",
    "\n",
    "print(f\"Dataset de test cargado con {len(test_dataset)} muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce45350",
   "metadata": {},
   "source": [
    "## 3. Realizar Predicciones y Visualizar Resultados\n",
    "\n",
    "Seleccionamos algunas muestras aleatorias y visualizamos las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7747e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(model, dataset, idx):\n",
    "    \"\"\"Visualiza la predicción para una muestra específica\"\"\"\n",
    "    inputs, target, timestamps = dataset[idx]\n",
    "    \n",
    "    # Preparar input para el modelo\n",
    "    inputs_batch = inputs.unsqueeze(0).to(model.device)\n",
    "    \n",
    "    # Realizar predicción\n",
    "    with torch.no_grad():\n",
    "        prediction = model(inputs_batch)\n",
    "    \n",
    "    # Mover a CPU y convertir a numpy\n",
    "    prediction = prediction.cpu().squeeze(0)\n",
    "    \n",
    "    # Desnormalizar si es necesario\n",
    "    inputs = inputs * 100.0\n",
    "    target = target * 100.0\n",
    "    prediction = prediction * 100.0\n",
    "    \n",
    "    # Configurar visualización\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Último frame de entrada\n",
    "    im1 = axes[0].imshow(inputs[-1].numpy(), cmap='RdBu_r', vmin=0, vmax=100)\n",
    "    axes[0].set_title(f'Input frame\\n{timestamps[-1]}')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Predicción\n",
    "    im2 = axes[1].imshow(prediction[0].numpy(), cmap='RdBu_r', vmin=0, vmax=100)\n",
    "    axes[1].set_title(f'Prediction\\n{timestamps[-1]}')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Target\n",
    "    im3 = axes[2].imshow(target[0].numpy(), cmap='RdBu_r', vmin=0, vmax=100)\n",
    "    axes[2].set_title(f'Target\\n{timestamps[-1]}')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Barra de color común\n",
    "    plt.colorbar(im2, ax=axes.ravel().tolist(), orientation='horizontal',\n",
    "                pad=0.01, fraction=0.05, label='Intensidad')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular métricas\n",
    "    metrics = calculate_metrics(\n",
    "        prediction[0],\n",
    "        target[0],\n",
    "        threshold=METRICS_CONFIG['threshold']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMétricas para esta predicción:\")\n",
    "    for metric, value in metrics.items():\n",
    "        value_float = value.item() if isinstance(value, torch.Tensor) else value\n",
    "        print(f\"{metric.upper()}: {value_float:.4f}\")\n",
    "    \n",
    "    return prediction, metrics\n",
    "\n",
    "# Visualizar algunas predicciones aleatorias\n",
    "n_samples = 3\n",
    "random_indices = np.random.choice(len(test_dataset), n_samples)\n",
    "\n",
    "for idx in random_indices:\n",
    "    print(f\"\\nMuestra {idx}:\")\n",
    "    prediction, metrics = visualize_prediction(model, test_dataset, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367de8a3",
   "metadata": {},
   "source": [
    "## 4. Evaluar Métricas en Todo el Conjunto de Test\n",
    "\n",
    "Calculamos las métricas en todo el conjunto de test para tener una evaluación más completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_full_test_set(model, dataset):\n",
    "    \"\"\"Evalúa el modelo en todo el conjunto de test\"\"\"\n",
    "    all_metrics = []\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        inputs, target, _ = dataset[idx]\n",
    "        inputs_batch = inputs.unsqueeze(0).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model(inputs_batch)\n",
    "        \n",
    "        prediction = prediction.cpu().squeeze(0)\n",
    "        metrics = calculate_metrics(\n",
    "            prediction[0],\n",
    "            target[0],\n",
    "            threshold=METRICS_CONFIG['threshold']\n",
    "        )\n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"Procesadas {idx + 1}/{len(dataset)} muestras...\")\n",
    "    \n",
    "    # Calcular promedios\n",
    "    avg_metrics = {}\n",
    "    for metric in all_metrics[0].keys():\n",
    "        values = [m[metric].item() if isinstance(m[metric], torch.Tensor) else m[metric] for m in all_metrics]\n",
    "        avg_metrics[metric] = {\n",
    "            'mean': np.mean(values),\n",
    "            'std': np.std(values)\n",
    "        }\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "print(\"Evaluando todo el conjunto de test...\")\n",
    "test_metrics = evaluate_full_test_set(model, test_dataset)\n",
    "\n",
    "print(\"\\nResultados finales en test:\")\n",
    "for metric, stats in test_metrics.items():\n",
    "    print(f\"{metric.upper()}:\")\n",
    "    print(f\"  Media: {stats['mean']:.4f}\")\n",
    "    print(f\"  Desv. Est.: {stats['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0852ecc",
   "metadata": {},
   "source": [
    "## 5. Guardar Resultados\n",
    "\n",
    "Guardamos los resultados de la evaluación para referencia futura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e0c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_dir = \"../logs/test_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = os.path.join(results_dir, f\"test_results_{timestamp}.txt\")\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    f.write(\"Resultados de Evaluación en Test\\n\")\n",
    "    f.write(\"===============================\\n\\n\")\n",
    "    f.write(f\"Modelo: {checkpoint_path}\\n\")\n",
    "    f.write(f\"Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Métricas:\\n\")\n",
    "    for metric, stats in test_metrics.items():\n",
    "        f.write(f\"\\n{metric.upper()}:\\n\")\n",
    "        f.write(f\"  Media: {stats['mean']:.4f}\\n\")\n",
    "        f.write(f\"  Desv. Est.: {stats['std']:.4f}\\n\")\n",
    "\n",
    "print(f\"\\nResultados guardados en: {results_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
